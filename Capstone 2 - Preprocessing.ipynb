{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64870d1b",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6b42566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import contractions as cont\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33725587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "df = pd.read_csv('Corona_NLP.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adb5f7",
   "metadata": {},
   "source": [
    "In its current form, the OriginalTweet column contains unstructured data in the form of Tweets. Most NLP classification algorithms, however, operate on word or document vectors and so it is necessary to vectorize each Tweet before preceding to the modelling stage. To aid in the process of vectorization, I defined three functions and applied them to the OriginalTweet column: preprocess, lemmatize_and_remove_ents, fix_spelling, and vectorize. These functions either remove any words that do not contribute to sentiment (e.g., hashtags, handles, links, proper names) or reduce noise by removing stopwords, proper names, and mispellings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d66cd0",
   "metadata": {},
   "source": [
    "The first function preforms several important tasks: it converts each Tweet to lower case; expands contractions; and removes a variety of non-essential information (hashtags, twitter handles, links, accented characters, non-alphabetic characters apart from the dash \"-\", and stopwords). The second step proved necessary to preserve the word \"not,\" which is often essential for distinguishing positive sentiments from negative ones. Consider, for example, the difference in meaning between 'happy' and 'not happy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e48c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('not') \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase\n",
    "    text = \" \".join([cont.fix(word) for word in text.split()]) #expand contractions\n",
    "    text = re.sub(r\"(#\\S+)\", '', text) #remove hashtags\n",
    "    text = re.sub(r\"(@\\S+)\", '', text) #remove handles\n",
    "    text = re.sub(r\"(http\\S+)\", '', text) #remove links\n",
    "    text = unicodedata.normalize('NFKD', text) #remove diacritics\n",
    "    text = text.encode('ascii', errors='ignore').decode('utf-8', errors='ignore') \n",
    "    tokens = [word.strip() for word in text.split() if word not in stopword_list] #remove stopwords\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"[^a-z ]+\", '', text) #remove special characters \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f63e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProcessedTweet'] = df['OriginalTweet'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a28fbe40",
   "metadata": {},
   "source": [
    "The second function lemmatizes the words in each Tweet and removes proper names. It requires a lot of memory, so I ran it separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0f3d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", exclude=['Parser'])\n",
    "\n",
    "def lemmatize_and_remove_ents(text):\n",
    "    text = nlp(text)\n",
    "    text = \" \".join([word.lemma_ if word.lemma_ != \"-PRON-\" else word.text for word in text if not word.ent_type_ ])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe711ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProcessedTweet'] = df['ProcessedTweet'].apply(lemmatize_and_remove_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a335f9b",
   "metadata": {},
   "source": [
    "The third and final function converts the processed tweets into document vectors using a combination of a pretrained GloVe embedding and Tfidf weighting. It does so by calculating the sum of individual word vectors multiplied by its Tfidf weight for each word within a Tweet. Weighting the individual word vectors helps ensure that the embeddings for long and short Tweets do not differ too drastically. \n",
    "\n",
    "Originally, I had planned use a larger embedding, such as the 'word2vec-google-news-300', but my computer couldn't handle it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cda6aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('glove-twitter-25')\n",
    "\n",
    "tv = TfidfVectorizer(stop_words=stopword_list)\n",
    "tv_transformed = tv.fit_transform(df['ProcessedTweet'])\n",
    "tfidf_values = dict(zip(tv.get_feature_names(), tv.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14612b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    vector = sum([glove[word]*tfidf_values[word] for word in text.split() if word in tfidf_values.keys() and word in glove.key_to_index])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "434fc6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "      <td>[-31.063377, 4.315184, 3.272048, -18.336933, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "      <td>[-18.462826, 11.85384, -2.6134028, 6.3497987, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect love one</td>\n",
       "      <td>[-2.7797158, 3.3976798, -5.5020814, 8.440672, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "      <td>[-40.63399, 35.210224, -24.598307, -49.416866,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "      <td>[-22.85473, 14.65009, 17.584171, 6.3308125, -3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                      ProcessedTweet  \\\n",
       "0  trend new yorkers encounter empty supermarket ...   \n",
       "1  could not find hand sanitizer fred meyer turn ...   \n",
       "2                              find protect love one   \n",
       "3  buy hit city anxious shopper stock foodampmedi...   \n",
       "4  everyone buy baby milk powder next everyone bu...   \n",
       "\n",
       "                                           DocVector  \n",
       "0  [-31.063377, 4.315184, 3.272048, -18.336933, 2...  \n",
       "1  [-18.462826, 11.85384, -2.6134028, 6.3497987, ...  \n",
       "2  [-2.7797158, 3.3976798, -5.5020814, 8.440672, ...  \n",
       "3  [-40.63399, 35.210224, -24.598307, -49.416866,...  \n",
       "4  [-22.85473, 14.65009, 17.584171, 6.3308125, -3...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DocVector'] = df['ProcessedTweet'].apply(vectorizer)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6011a",
   "metadata": {},
   "source": [
    "Next I need to transform DocVector from a column of lists into a series of separate columns, one for each of the 25 vectorized features. This dataframe will contain all of the explanatory variables for the modelling step of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c924e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 15</th>\n",
       "      <th>Feature 16</th>\n",
       "      <th>Feature 17</th>\n",
       "      <th>Feature 18</th>\n",
       "      <th>Feature 19</th>\n",
       "      <th>Feature 20</th>\n",
       "      <th>Feature 21</th>\n",
       "      <th>Feature 22</th>\n",
       "      <th>Feature 23</th>\n",
       "      <th>Feature 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-31.063377</td>\n",
       "      <td>4.315184</td>\n",
       "      <td>3.272048</td>\n",
       "      <td>-18.336933</td>\n",
       "      <td>20.507353</td>\n",
       "      <td>-18.973701</td>\n",
       "      <td>39.690273</td>\n",
       "      <td>-63.898300</td>\n",
       "      <td>24.546728</td>\n",
       "      <td>-10.839511</td>\n",
       "      <td>...</td>\n",
       "      <td>12.867351</td>\n",
       "      <td>36.950283</td>\n",
       "      <td>-19.829515</td>\n",
       "      <td>22.515028</td>\n",
       "      <td>1.602518</td>\n",
       "      <td>-13.314288</td>\n",
       "      <td>-18.912729</td>\n",
       "      <td>-7.682436</td>\n",
       "      <td>-29.146791</td>\n",
       "      <td>-32.590302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-18.462826</td>\n",
       "      <td>11.853840</td>\n",
       "      <td>-2.613403</td>\n",
       "      <td>6.349799</td>\n",
       "      <td>-18.324095</td>\n",
       "      <td>-4.529284</td>\n",
       "      <td>22.416130</td>\n",
       "      <td>-37.670242</td>\n",
       "      <td>22.033010</td>\n",
       "      <td>-21.450129</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200512</td>\n",
       "      <td>8.932176</td>\n",
       "      <td>-8.927069</td>\n",
       "      <td>15.534106</td>\n",
       "      <td>-15.571704</td>\n",
       "      <td>-4.428163</td>\n",
       "      <td>11.713925</td>\n",
       "      <td>12.254540</td>\n",
       "      <td>5.701349</td>\n",
       "      <td>-26.367716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.779716</td>\n",
       "      <td>3.397680</td>\n",
       "      <td>-5.502081</td>\n",
       "      <td>8.440672</td>\n",
       "      <td>-8.699121</td>\n",
       "      <td>-5.226532</td>\n",
       "      <td>29.461939</td>\n",
       "      <td>2.573042</td>\n",
       "      <td>-7.642838</td>\n",
       "      <td>0.953844</td>\n",
       "      <td>...</td>\n",
       "      <td>4.212283</td>\n",
       "      <td>5.015493</td>\n",
       "      <td>-11.010517</td>\n",
       "      <td>5.611660</td>\n",
       "      <td>-7.700294</td>\n",
       "      <td>-2.587625</td>\n",
       "      <td>-0.576298</td>\n",
       "      <td>-5.781661</td>\n",
       "      <td>0.555235</td>\n",
       "      <td>-8.109362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-40.633991</td>\n",
       "      <td>35.210224</td>\n",
       "      <td>-24.598307</td>\n",
       "      <td>-49.416866</td>\n",
       "      <td>8.011915</td>\n",
       "      <td>7.370157</td>\n",
       "      <td>49.256386</td>\n",
       "      <td>-25.614607</td>\n",
       "      <td>34.770287</td>\n",
       "      <td>-19.868443</td>\n",
       "      <td>...</td>\n",
       "      <td>6.071897</td>\n",
       "      <td>28.622406</td>\n",
       "      <td>-23.586481</td>\n",
       "      <td>-16.194962</td>\n",
       "      <td>-31.503876</td>\n",
       "      <td>-27.649782</td>\n",
       "      <td>-26.429310</td>\n",
       "      <td>-12.657497</td>\n",
       "      <td>-22.968901</td>\n",
       "      <td>-21.575285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-22.854731</td>\n",
       "      <td>14.650090</td>\n",
       "      <td>17.584171</td>\n",
       "      <td>6.330812</td>\n",
       "      <td>-3.588906</td>\n",
       "      <td>6.745114</td>\n",
       "      <td>55.977005</td>\n",
       "      <td>-31.296013</td>\n",
       "      <td>4.496396</td>\n",
       "      <td>18.477859</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.855762</td>\n",
       "      <td>31.614111</td>\n",
       "      <td>-41.765991</td>\n",
       "      <td>2.643752</td>\n",
       "      <td>8.316241</td>\n",
       "      <td>-27.605976</td>\n",
       "      <td>-7.106748</td>\n",
       "      <td>15.256669</td>\n",
       "      <td>24.210051</td>\n",
       "      <td>-8.261431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 0  Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  \\\n",
       "0 -31.063377   4.315184   3.272048 -18.336933  20.507353 -18.973701   \n",
       "1 -18.462826  11.853840  -2.613403   6.349799 -18.324095  -4.529284   \n",
       "2  -2.779716   3.397680  -5.502081   8.440672  -8.699121  -5.226532   \n",
       "3 -40.633991  35.210224 -24.598307 -49.416866   8.011915   7.370157   \n",
       "4 -22.854731  14.650090  17.584171   6.330812  -3.588906   6.745114   \n",
       "\n",
       "   Feature 6  Feature 7  Feature 8  Feature 9  ...  Feature 15  Feature 16  \\\n",
       "0  39.690273 -63.898300  24.546728 -10.839511  ...   12.867351   36.950283   \n",
       "1  22.416130 -37.670242  22.033010 -21.450129  ...   -1.200512    8.932176   \n",
       "2  29.461939   2.573042  -7.642838   0.953844  ...    4.212283    5.015493   \n",
       "3  49.256386 -25.614607  34.770287 -19.868443  ...    6.071897   28.622406   \n",
       "4  55.977005 -31.296013   4.496396  18.477859  ...   -7.855762   31.614111   \n",
       "\n",
       "   Feature 17  Feature 18  Feature 19  Feature 20  Feature 21  Feature 22  \\\n",
       "0  -19.829515   22.515028    1.602518  -13.314288  -18.912729   -7.682436   \n",
       "1   -8.927069   15.534106  -15.571704   -4.428163   11.713925   12.254540   \n",
       "2  -11.010517    5.611660   -7.700294   -2.587625   -0.576298   -5.781661   \n",
       "3  -23.586481  -16.194962  -31.503876  -27.649782  -26.429310  -12.657497   \n",
       "4  -41.765991    2.643752    8.316241  -27.605976   -7.106748   15.256669   \n",
       "\n",
       "   Feature 23  Feature 24  \n",
       "0  -29.146791  -32.590302  \n",
       "1    5.701349  -26.367716  \n",
       "2    0.555235   -8.109362  \n",
       "3  -22.968901  -21.575285  \n",
       "4   24.210051   -8.261431  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.DocVector.apply(pd.Series)\n",
    "X.columns = ['Feature ' + str(i) for i in range(25)]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2fee452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "      <td>[-31.063377, 4.315184, 3.272048, -18.336933, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "      <td>[-18.462826, 11.85384, -2.6134028, 6.3497987, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect love one</td>\n",
       "      <td>[-2.7797158, 3.3976798, -5.5020814, 8.440672, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "      <td>[-40.63399, 35.210224, -24.598307, -49.416866,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "      <td>[-22.85473, 14.65009, 17.584171, 6.3308125, -3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                      OriginalTweet  \\\n",
       "0      0  TRENDING: New Yorkers encounter empty supermar...   \n",
       "1      1  When I couldn't find hand sanitizer at Fred Me...   \n",
       "2      2  Find out how you can protect yourself and love...   \n",
       "3      3  #Panic buying hits #NewYork City as anxious sh...   \n",
       "4      4  #toiletpaper #dunnypaper #coronavirus #coronav...   \n",
       "\n",
       "            Sentiment                                     ProcessedTweet  \\\n",
       "0  Extremely Negative  trend new yorkers encounter empty supermarket ...   \n",
       "1            Positive  could not find hand sanitizer fred meyer turn ...   \n",
       "2  Extremely Positive                              find protect love one   \n",
       "3            Negative  buy hit city anxious shopper stock foodampmedi...   \n",
       "4             Neutral  everyone buy baby milk powder next everyone bu...   \n",
       "\n",
       "                                           DocVector  \n",
       "0  [-31.063377, 4.315184, 3.272048, -18.336933, 2...  \n",
       "1  [-18.462826, 11.85384, -2.6134028, 6.3497987, ...  \n",
       "2  [-2.7797158, 3.3976798, -5.5020814, 8.440672, ...  \n",
       "3  [-40.63399, 35.210224, -24.598307, -49.416866,...  \n",
       "4  [-22.85473, 14.65009, 17.584171, 6.3308125, -3...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reset_index(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.merge(X, how='outer', on='index')\n",
    "df.drop(columns=['level_0', 'index'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89acf5ca",
   "metadata": {},
   "source": [
    "Unfortunately, the preprocessing steps introduced some NaNs into the data where the original Tweet consisted of a combination of hashtags, handles, links, stopwords, and words not found in the twitter-glove-25 dictionary. These entries need to be removed before modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75439132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 15</th>\n",
       "      <th>Feature 16</th>\n",
       "      <th>Feature 17</th>\n",
       "      <th>Feature 18</th>\n",
       "      <th>Feature 19</th>\n",
       "      <th>Feature 20</th>\n",
       "      <th>Feature 21</th>\n",
       "      <th>Feature 22</th>\n",
       "      <th>Feature 23</th>\n",
       "      <th>Feature 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>Il #coronavirus colpisce maggiormente i polmon...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>I've #never #seen so may #men in a #supermarke...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>#Coronavirus #preparation: What to #stock-up o...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>@Janetb172 @denyessence @NoScienceDenial @west...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>@KrampusFu @JackHer18731941 @Twistagirl1958 @W...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42726</th>\n",
       "      <td>Nightcrawler #nightcrawler #thespot #scoop #po...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43245</th>\n",
       "      <td>Adel and Karina xxx\\r\\r\\n\\r\\r\\nhttps://t.co/E3...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44280</th>\n",
       "      <td>@jamisonglory @DailyMail This is what #antifa ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44323</th>\n",
       "      <td>It is a great week again \\r\\r\\n#mondaymotivati...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44910</th>\n",
       "      <td>#Coronavirus ?? ????? ??? ????? ?? ??? ???????...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment  \\\n",
       "1821   Il #coronavirus colpisce maggiormente i polmon...             Neutral   \n",
       "2598   I've #never #seen so may #men in a #supermarke...             Neutral   \n",
       "2729   #Coronavirus #preparation: What to #stock-up o...             Neutral   \n",
       "3063   @Janetb172 @denyessence @NoScienceDenial @west...             Neutral   \n",
       "3192   @KrampusFu @JackHer18731941 @Twistagirl1958 @W...             Neutral   \n",
       "...                                                  ...                 ...   \n",
       "42726  Nightcrawler #nightcrawler #thespot #scoop #po...             Neutral   \n",
       "43245  Adel and Karina xxx\\r\\r\\n\\r\\r\\nhttps://t.co/E3...             Neutral   \n",
       "44280  @jamisonglory @DailyMail This is what #antifa ...             Neutral   \n",
       "44323  It is a great week again \\r\\r\\n#mondaymotivati...  Extremely Positive   \n",
       "44910  #Coronavirus ?? ????? ??? ????? ?? ??? ???????...             Neutral   \n",
       "\n",
       "                     ProcessedTweet DocVector  Feature 0  Feature 1  \\\n",
       "1821                                        0        0.0        NaN   \n",
       "2598                                        0        0.0        NaN   \n",
       "2729                                        0        0.0        NaN   \n",
       "3063                                        0        0.0        NaN   \n",
       "3192                                        0        0.0        NaN   \n",
       "...                             ...       ...        ...        ...   \n",
       "42726                                       0        0.0        NaN   \n",
       "43245                                       0        0.0        NaN   \n",
       "44280                                       0        0.0        NaN   \n",
       "44323                                       0        0.0        NaN   \n",
       "44910                                       0        0.0        NaN   \n",
       "\n",
       "       Feature 2  Feature 3  Feature 4  Feature 5  ...  Feature 15  \\\n",
       "1821         NaN        NaN        NaN        NaN  ...         NaN   \n",
       "2598         NaN        NaN        NaN        NaN  ...         NaN   \n",
       "2729         NaN        NaN        NaN        NaN  ...         NaN   \n",
       "3063         NaN        NaN        NaN        NaN  ...         NaN   \n",
       "3192         NaN        NaN        NaN        NaN  ...         NaN   \n",
       "...          ...        ...        ...        ...  ...         ...   \n",
       "42726        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "43245        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "44280        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "44323        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "44910        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "\n",
       "       Feature 16  Feature 17  Feature 18  Feature 19  Feature 20  Feature 21  \\\n",
       "1821          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2598          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2729          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3063          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3192          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "42726         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "43245         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "44280         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "44323         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "44910         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "       Feature 22  Feature 23  Feature 24  \n",
       "1821          NaN         NaN         NaN  \n",
       "2598          NaN         NaN         NaN  \n",
       "2729          NaN         NaN         NaN  \n",
       "3063          NaN         NaN         NaN  \n",
       "3192          NaN         NaN         NaN  \n",
       "...           ...         ...         ...  \n",
       "42726         NaN         NaN         NaN  \n",
       "43245         NaN         NaN         NaN  \n",
       "44280         NaN         NaN         NaN  \n",
       "44323         NaN         NaN         NaN  \n",
       "44910         NaN         NaN         NaN  \n",
       "\n",
       "[128 rows x 29 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Feature 1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af5692b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 15</th>\n",
       "      <th>Feature 16</th>\n",
       "      <th>Feature 17</th>\n",
       "      <th>Feature 18</th>\n",
       "      <th>Feature 19</th>\n",
       "      <th>Feature 20</th>\n",
       "      <th>Feature 21</th>\n",
       "      <th>Feature 22</th>\n",
       "      <th>Feature 23</th>\n",
       "      <th>Feature 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "      <td>[-31.063377, 4.315184, 3.272048, -18.336933, 2...</td>\n",
       "      <td>-31.063377</td>\n",
       "      <td>4.315184</td>\n",
       "      <td>3.272048</td>\n",
       "      <td>-18.336933</td>\n",
       "      <td>20.507353</td>\n",
       "      <td>-18.973701</td>\n",
       "      <td>...</td>\n",
       "      <td>12.867351</td>\n",
       "      <td>36.950283</td>\n",
       "      <td>-19.829515</td>\n",
       "      <td>22.515028</td>\n",
       "      <td>1.602518</td>\n",
       "      <td>-13.314288</td>\n",
       "      <td>-18.912729</td>\n",
       "      <td>-7.682436</td>\n",
       "      <td>-29.146791</td>\n",
       "      <td>-32.590302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "      <td>[-18.462826, 11.85384, -2.6134028, 6.3497987, ...</td>\n",
       "      <td>-18.462826</td>\n",
       "      <td>11.853840</td>\n",
       "      <td>-2.613403</td>\n",
       "      <td>6.349799</td>\n",
       "      <td>-18.324095</td>\n",
       "      <td>-4.529284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200512</td>\n",
       "      <td>8.932176</td>\n",
       "      <td>-8.927069</td>\n",
       "      <td>15.534106</td>\n",
       "      <td>-15.571704</td>\n",
       "      <td>-4.428163</td>\n",
       "      <td>11.713925</td>\n",
       "      <td>12.254540</td>\n",
       "      <td>5.701349</td>\n",
       "      <td>-26.367716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect love one</td>\n",
       "      <td>[-2.7797158, 3.3976798, -5.5020814, 8.440672, ...</td>\n",
       "      <td>-2.779716</td>\n",
       "      <td>3.397680</td>\n",
       "      <td>-5.502081</td>\n",
       "      <td>8.440672</td>\n",
       "      <td>-8.699121</td>\n",
       "      <td>-5.226532</td>\n",
       "      <td>...</td>\n",
       "      <td>4.212283</td>\n",
       "      <td>5.015493</td>\n",
       "      <td>-11.010517</td>\n",
       "      <td>5.611660</td>\n",
       "      <td>-7.700294</td>\n",
       "      <td>-2.587625</td>\n",
       "      <td>-0.576298</td>\n",
       "      <td>-5.781661</td>\n",
       "      <td>0.555235</td>\n",
       "      <td>-8.109362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "      <td>[-40.63399, 35.210224, -24.598307, -49.416866,...</td>\n",
       "      <td>-40.633991</td>\n",
       "      <td>35.210224</td>\n",
       "      <td>-24.598307</td>\n",
       "      <td>-49.416866</td>\n",
       "      <td>8.011915</td>\n",
       "      <td>7.370157</td>\n",
       "      <td>...</td>\n",
       "      <td>6.071897</td>\n",
       "      <td>28.622406</td>\n",
       "      <td>-23.586481</td>\n",
       "      <td>-16.194962</td>\n",
       "      <td>-31.503876</td>\n",
       "      <td>-27.649782</td>\n",
       "      <td>-26.429310</td>\n",
       "      <td>-12.657497</td>\n",
       "      <td>-22.968901</td>\n",
       "      <td>-21.575285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "      <td>[-22.85473, 14.65009, 17.584171, 6.3308125, -3...</td>\n",
       "      <td>-22.854731</td>\n",
       "      <td>14.650090</td>\n",
       "      <td>17.584171</td>\n",
       "      <td>6.330812</td>\n",
       "      <td>-3.588906</td>\n",
       "      <td>6.745114</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.855762</td>\n",
       "      <td>31.614111</td>\n",
       "      <td>-41.765991</td>\n",
       "      <td>2.643752</td>\n",
       "      <td>8.316241</td>\n",
       "      <td>-27.605976</td>\n",
       "      <td>-7.106748</td>\n",
       "      <td>15.256669</td>\n",
       "      <td>24.210051</td>\n",
       "      <td>-8.261431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                      ProcessedTweet  \\\n",
       "0  trend new yorkers encounter empty supermarket ...   \n",
       "1  could not find hand sanitizer fred meyer turn ...   \n",
       "2                              find protect love one   \n",
       "3  buy hit city anxious shopper stock foodampmedi...   \n",
       "4  everyone buy baby milk powder next everyone bu...   \n",
       "\n",
       "                                           DocVector  Feature 0  Feature 1  \\\n",
       "0  [-31.063377, 4.315184, 3.272048, -18.336933, 2... -31.063377   4.315184   \n",
       "1  [-18.462826, 11.85384, -2.6134028, 6.3497987, ... -18.462826  11.853840   \n",
       "2  [-2.7797158, 3.3976798, -5.5020814, 8.440672, ...  -2.779716   3.397680   \n",
       "3  [-40.63399, 35.210224, -24.598307, -49.416866,... -40.633991  35.210224   \n",
       "4  [-22.85473, 14.65009, 17.584171, 6.3308125, -3... -22.854731  14.650090   \n",
       "\n",
       "   Feature 2  Feature 3  Feature 4  Feature 5  ...  Feature 15  Feature 16  \\\n",
       "0   3.272048 -18.336933  20.507353 -18.973701  ...   12.867351   36.950283   \n",
       "1  -2.613403   6.349799 -18.324095  -4.529284  ...   -1.200512    8.932176   \n",
       "2  -5.502081   8.440672  -8.699121  -5.226532  ...    4.212283    5.015493   \n",
       "3 -24.598307 -49.416866   8.011915   7.370157  ...    6.071897   28.622406   \n",
       "4  17.584171   6.330812  -3.588906   6.745114  ...   -7.855762   31.614111   \n",
       "\n",
       "   Feature 17  Feature 18  Feature 19  Feature 20  Feature 21  Feature 22  \\\n",
       "0  -19.829515   22.515028    1.602518  -13.314288  -18.912729   -7.682436   \n",
       "1   -8.927069   15.534106  -15.571704   -4.428163   11.713925   12.254540   \n",
       "2  -11.010517    5.611660   -7.700294   -2.587625   -0.576298   -5.781661   \n",
       "3  -23.586481  -16.194962  -31.503876  -27.649782  -26.429310  -12.657497   \n",
       "4  -41.765991    2.643752    8.316241  -27.605976   -7.106748   15.256669   \n",
       "\n",
       "   Feature 23  Feature 24  \n",
       "0  -29.146791  -32.590302  \n",
       "1    5.701349  -26.367716  \n",
       "2    0.555235   -8.109362  \n",
       "3  -22.968901  -21.575285  \n",
       "4   24.210051   -8.261431  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()\n",
    "df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb07068",
   "metadata": {},
   "source": [
    "Next I need to replace the sentiment labels with numerical dummy values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00d1ee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 15</th>\n",
       "      <th>Feature 16</th>\n",
       "      <th>Feature 17</th>\n",
       "      <th>Feature 18</th>\n",
       "      <th>Feature 19</th>\n",
       "      <th>Feature 20</th>\n",
       "      <th>Feature 21</th>\n",
       "      <th>Feature 22</th>\n",
       "      <th>Feature 23</th>\n",
       "      <th>Feature 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>0</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "      <td>[-31.063377, 4.315184, 3.272048, -18.336933, 2...</td>\n",
       "      <td>-31.063377</td>\n",
       "      <td>4.315184</td>\n",
       "      <td>3.272048</td>\n",
       "      <td>-18.336933</td>\n",
       "      <td>20.507353</td>\n",
       "      <td>-18.973701</td>\n",
       "      <td>...</td>\n",
       "      <td>12.867351</td>\n",
       "      <td>36.950283</td>\n",
       "      <td>-19.829515</td>\n",
       "      <td>22.515028</td>\n",
       "      <td>1.602518</td>\n",
       "      <td>-13.314288</td>\n",
       "      <td>-18.912729</td>\n",
       "      <td>-7.682436</td>\n",
       "      <td>-29.146791</td>\n",
       "      <td>-32.590302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>3</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "      <td>[-18.462826, 11.85384, -2.6134028, 6.3497987, ...</td>\n",
       "      <td>-18.462826</td>\n",
       "      <td>11.853840</td>\n",
       "      <td>-2.613403</td>\n",
       "      <td>6.349799</td>\n",
       "      <td>-18.324095</td>\n",
       "      <td>-4.529284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200512</td>\n",
       "      <td>8.932176</td>\n",
       "      <td>-8.927069</td>\n",
       "      <td>15.534106</td>\n",
       "      <td>-15.571704</td>\n",
       "      <td>-4.428163</td>\n",
       "      <td>11.713925</td>\n",
       "      <td>12.254540</td>\n",
       "      <td>5.701349</td>\n",
       "      <td>-26.367716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>4</td>\n",
       "      <td>find protect love one</td>\n",
       "      <td>[-2.7797158, 3.3976798, -5.5020814, 8.440672, ...</td>\n",
       "      <td>-2.779716</td>\n",
       "      <td>3.397680</td>\n",
       "      <td>-5.502081</td>\n",
       "      <td>8.440672</td>\n",
       "      <td>-8.699121</td>\n",
       "      <td>-5.226532</td>\n",
       "      <td>...</td>\n",
       "      <td>4.212283</td>\n",
       "      <td>5.015493</td>\n",
       "      <td>-11.010517</td>\n",
       "      <td>5.611660</td>\n",
       "      <td>-7.700294</td>\n",
       "      <td>-2.587625</td>\n",
       "      <td>-0.576298</td>\n",
       "      <td>-5.781661</td>\n",
       "      <td>0.555235</td>\n",
       "      <td>-8.109362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "      <td>[-40.63399, 35.210224, -24.598307, -49.416866,...</td>\n",
       "      <td>-40.633991</td>\n",
       "      <td>35.210224</td>\n",
       "      <td>-24.598307</td>\n",
       "      <td>-49.416866</td>\n",
       "      <td>8.011915</td>\n",
       "      <td>7.370157</td>\n",
       "      <td>...</td>\n",
       "      <td>6.071897</td>\n",
       "      <td>28.622406</td>\n",
       "      <td>-23.586481</td>\n",
       "      <td>-16.194962</td>\n",
       "      <td>-31.503876</td>\n",
       "      <td>-27.649782</td>\n",
       "      <td>-26.429310</td>\n",
       "      <td>-12.657497</td>\n",
       "      <td>-22.968901</td>\n",
       "      <td>-21.575285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>2</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "      <td>[-22.85473, 14.65009, 17.584171, 6.3308125, -3...</td>\n",
       "      <td>-22.854731</td>\n",
       "      <td>14.650090</td>\n",
       "      <td>17.584171</td>\n",
       "      <td>6.330812</td>\n",
       "      <td>-3.588906</td>\n",
       "      <td>6.745114</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.855762</td>\n",
       "      <td>31.614111</td>\n",
       "      <td>-41.765991</td>\n",
       "      <td>2.643752</td>\n",
       "      <td>8.316241</td>\n",
       "      <td>-27.605976</td>\n",
       "      <td>-7.106748</td>\n",
       "      <td>15.256669</td>\n",
       "      <td>24.210051</td>\n",
       "      <td>-8.261431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...          0   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...          3   \n",
       "2  Find out how you can protect yourself and love...          4   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...          1   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...          2   \n",
       "\n",
       "                                      ProcessedTweet  \\\n",
       "0  trend new yorkers encounter empty supermarket ...   \n",
       "1  could not find hand sanitizer fred meyer turn ...   \n",
       "2                              find protect love one   \n",
       "3  buy hit city anxious shopper stock foodampmedi...   \n",
       "4  everyone buy baby milk powder next everyone bu...   \n",
       "\n",
       "                                           DocVector  Feature 0  Feature 1  \\\n",
       "0  [-31.063377, 4.315184, 3.272048, -18.336933, 2... -31.063377   4.315184   \n",
       "1  [-18.462826, 11.85384, -2.6134028, 6.3497987, ... -18.462826  11.853840   \n",
       "2  [-2.7797158, 3.3976798, -5.5020814, 8.440672, ...  -2.779716   3.397680   \n",
       "3  [-40.63399, 35.210224, -24.598307, -49.416866,... -40.633991  35.210224   \n",
       "4  [-22.85473, 14.65009, 17.584171, 6.3308125, -3... -22.854731  14.650090   \n",
       "\n",
       "   Feature 2  Feature 3  Feature 4  Feature 5  ...  Feature 15  Feature 16  \\\n",
       "0   3.272048 -18.336933  20.507353 -18.973701  ...   12.867351   36.950283   \n",
       "1  -2.613403   6.349799 -18.324095  -4.529284  ...   -1.200512    8.932176   \n",
       "2  -5.502081   8.440672  -8.699121  -5.226532  ...    4.212283    5.015493   \n",
       "3 -24.598307 -49.416866   8.011915   7.370157  ...    6.071897   28.622406   \n",
       "4  17.584171   6.330812  -3.588906   6.745114  ...   -7.855762   31.614111   \n",
       "\n",
       "   Feature 17  Feature 18  Feature 19  Feature 20  Feature 21  Feature 22  \\\n",
       "0  -19.829515   22.515028    1.602518  -13.314288  -18.912729   -7.682436   \n",
       "1   -8.927069   15.534106  -15.571704   -4.428163   11.713925   12.254540   \n",
       "2  -11.010517    5.611660   -7.700294   -2.587625   -0.576298   -5.781661   \n",
       "3  -23.586481  -16.194962  -31.503876  -27.649782  -26.429310  -12.657497   \n",
       "4  -41.765991    2.643752    8.316241  -27.605976   -7.106748   15.256669   \n",
       "\n",
       "   Feature 23  Feature 24  \n",
       "0  -29.146791  -32.590302  \n",
       "1    5.701349  -26.367716  \n",
       "2    0.555235   -8.109362  \n",
       "3  -22.968901  -21.575285  \n",
       "4   24.210051   -8.261431  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements = {'Extremely Negative': 0, 'Negative': 1, 'Neutral' : 2, 'Positive' : 3, 'Extremely Positive' : 4}\n",
    "df['Sentiment'].replace(replacements, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edcd24",
   "metadata": {},
   "source": [
    "The final step in preprocessing is to divide the data into test and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c3e92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Sentiment']\n",
    "X = df.drop(columns=['OriginalTweet', 'Sentiment', 'ProcessedTweet', 'DocVector'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6393dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_Corona_NLP.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
