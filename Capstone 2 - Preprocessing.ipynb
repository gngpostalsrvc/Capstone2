{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64870d1b",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b42566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import contractions as cont\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33725587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative\n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive\n",
       "2  Find out how you can protect yourself and love...  Extremely Positive\n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative\n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "df = pd.read_csv('Corona_NLP.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2776b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) #Something wonky happened while saving the dataframe to csv and two of the Sentiments were\n",
    "#saved as separate rows, creating a few NaNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adb5f7",
   "metadata": {},
   "source": [
    "In its current form, the OriginalTweet column contains unstructured data in the form of Tweets. Most NLP classification algorithms, however, operate on word or document vectors and so it is necessary to vectorize each Tweet before preceding to the modelling stage. To aid in the process of vectorization, I defined three functions and applied them to the OriginalTweet column: preprocess, lemmatize_and_remove_ents, fix_spelling, and vectorize. These functions either remove any words that do not contribute to sentiment (e.g., hashtags, handles, links, proper names) or reduce noise by removing stopwords, proper names, and mispellings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d66cd0",
   "metadata": {},
   "source": [
    "The first function preforms several important tasks: it converts each Tweet to lower case; expands contractions; and removes a variety of non-essential information (hashtags, twitter handles, links, accented characters, non-alphabetic characters apart from the dash \"-\", and stopwords). The second step proved necessary to preserve the word \"not,\" which is often essential for distinguishing positive sentiments from negative ones. Consider, for example, the difference in meaning between 'happy' and 'not happy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e48c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('not') \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase\n",
    "    text = \" \".join([cont.fix(word) for word in text.split()]) #expand contractions\n",
    "    text = re.sub(r\"(#\\S+)\", '', text) #remove hashtags\n",
    "    text = re.sub(r\"(@\\S+)\", '', text) #remove handles\n",
    "    text = re.sub(r\"(http\\S+)\", '', text) #remove links\n",
    "    text = unicodedata.normalize('NFKD', text) #remove diacritics\n",
    "    text = text.encode('ascii', errors='ignore').decode('utf-8', errors='ignore') \n",
    "    tokens = [word.strip() for word in text.split() if word not in stopword_list] #remove stopwords\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"[^a-z- ]+\", '', text) #remove special characters \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f63e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could not find hand sanitizer fred meyer turne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect loved ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buying hits city anxious shoppers stock foodam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>one week everyone buying baby milk powder next...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                      ProcessedTweet  \n",
       "0  trending new yorkers encounter empty supermark...  \n",
       "1  could not find hand sanitizer fred meyer turne...  \n",
       "2                           find protect loved ones   \n",
       "3  buying hits city anxious shoppers stock foodam...  \n",
       "4  one week everyone buying baby milk powder next...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ProcessedTweet'] = df['OriginalTweet'].apply(preprocess)\n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a28fbe40",
   "metadata": {},
   "source": [
    "The second function lemmatizes the words in each Tweet and removes proper names. It requires a lot of memory, so I ran it separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f3d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", exclude=['Parser'])\n",
    "\n",
    "def lemmatize_and_remove_ents(text):\n",
    "    text = nlp(text)\n",
    "    text = \" \".join([word.lemma_ if word.lemma_ != \"-PRON-\" else word.text for word in text if not word.ent_type_ ])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe711ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect love one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                      ProcessedTweet  \n",
       "0  trend new yorkers encounter empty supermarket ...  \n",
       "1  could not find hand sanitizer fred meyer turn ...  \n",
       "2                              find protect love one  \n",
       "3  buy hit city anxious shopper stock foodampmedi...  \n",
       "4  everyone buy baby milk powder next everyone bu...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ProcessedTweet'] = df['ProcessedTweet'].apply(lemmatize_and_remove_ents)\n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a335f9b",
   "metadata": {},
   "source": [
    "The third and final function converts the processed tweets into document vectors using a combination of a pretrained GloVe embedding and Tfidf weighting. It does so by calculating the sum of individual word vectors multiplied by its Tfidf weight for each word within a Tweet. Weighting the individual word vectors helps ensure that the embeddings for long and short Tweets do not differ too drastically. \n",
    "\n",
    "Originally, I had planned use a larger embedding, such as the 'word2vec-google-news-300', but my computer couldn't handle it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda6aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('glove-twitter-25')\n",
    "\n",
    "tv = TfidfVectorizer(stop_words=stopword_list)\n",
    "tv_transformed = tv.fit_transform(df['ProcessedTweet'])\n",
    "tfidf_values = dict(zip(tv.get_feature_names(), tv.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14612b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    vector = sum([glove[word]*tfidf_values[word] for word in text.split() if word in tfidf_values.keys() and word in glove.key_to_index])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434fc6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "      <td>[-25.564545, 12.41317, 0.9165025, -23.523197, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "      <td>[-18.407335, 11.812518, -2.667809, 6.388415, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect love one</td>\n",
       "      <td>[-2.7793016, 3.3964226, -5.5020514, 8.437631, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "      <td>[-40.624233, 35.157276, -24.591461, -49.36236,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "      <td>[-22.856718, 14.615568, 17.575512, 6.351932, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                      ProcessedTweet  \\\n",
       "0  trend new yorkers encounter empty supermarket ...   \n",
       "1  could not find hand sanitizer fred meyer turn ...   \n",
       "2                              find protect love one   \n",
       "3  buy hit city anxious shopper stock foodampmedi...   \n",
       "4  everyone buy baby milk powder next everyone bu...   \n",
       "\n",
       "                                           DocVector  \n",
       "0  [-25.564545, 12.41317, 0.9165025, -23.523197, ...  \n",
       "1  [-18.407335, 11.812518, -2.667809, 6.388415, -...  \n",
       "2  [-2.7793016, 3.3964226, -5.5020514, 8.437631, ...  \n",
       "3  [-40.624233, 35.157276, -24.591461, -49.36236,...  \n",
       "4  [-22.856718, 14.615568, 17.575512, 6.351932, -...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DocVector'] = df['ProcessedTweet'].apply(vectorizer)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6011a",
   "metadata": {},
   "source": [
    "Next I need to transform DocVector from a column of lists into a separate dataframe consisting of 25 columns, one for each of the 25 vectorized features. This dataframe will contain all of the explanatory variables for the modelling step of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c924e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 15</th>\n",
       "      <th>Feature 16</th>\n",
       "      <th>Feature 17</th>\n",
       "      <th>Feature 18</th>\n",
       "      <th>Feature 19</th>\n",
       "      <th>Feature 20</th>\n",
       "      <th>Feature 21</th>\n",
       "      <th>Feature 22</th>\n",
       "      <th>Feature 23</th>\n",
       "      <th>Feature 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-25.564545</td>\n",
       "      <td>12.413170</td>\n",
       "      <td>0.916502</td>\n",
       "      <td>-23.523197</td>\n",
       "      <td>23.942314</td>\n",
       "      <td>-19.245277</td>\n",
       "      <td>39.597092</td>\n",
       "      <td>-67.066742</td>\n",
       "      <td>27.730808</td>\n",
       "      <td>-9.485415</td>\n",
       "      <td>...</td>\n",
       "      <td>12.364698</td>\n",
       "      <td>41.769032</td>\n",
       "      <td>-26.896332</td>\n",
       "      <td>23.701748</td>\n",
       "      <td>-1.653188</td>\n",
       "      <td>-19.830172</td>\n",
       "      <td>-27.282768</td>\n",
       "      <td>-11.635708</td>\n",
       "      <td>-29.070320</td>\n",
       "      <td>-34.822224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-18.407335</td>\n",
       "      <td>11.812518</td>\n",
       "      <td>-2.667809</td>\n",
       "      <td>6.388415</td>\n",
       "      <td>-18.285372</td>\n",
       "      <td>-4.658720</td>\n",
       "      <td>22.325075</td>\n",
       "      <td>-37.602386</td>\n",
       "      <td>21.859562</td>\n",
       "      <td>-21.498049</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.258633</td>\n",
       "      <td>8.867926</td>\n",
       "      <td>-8.832096</td>\n",
       "      <td>15.507577</td>\n",
       "      <td>-15.484630</td>\n",
       "      <td>-4.390882</td>\n",
       "      <td>11.780195</td>\n",
       "      <td>12.107148</td>\n",
       "      <td>5.713201</td>\n",
       "      <td>-26.333225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.779302</td>\n",
       "      <td>3.396423</td>\n",
       "      <td>-5.502051</td>\n",
       "      <td>8.437631</td>\n",
       "      <td>-8.695304</td>\n",
       "      <td>-5.224719</td>\n",
       "      <td>29.446379</td>\n",
       "      <td>2.569920</td>\n",
       "      <td>-7.636728</td>\n",
       "      <td>0.954528</td>\n",
       "      <td>...</td>\n",
       "      <td>4.210380</td>\n",
       "      <td>5.014014</td>\n",
       "      <td>-11.005681</td>\n",
       "      <td>5.610911</td>\n",
       "      <td>-7.698394</td>\n",
       "      <td>-2.586307</td>\n",
       "      <td>-0.576156</td>\n",
       "      <td>-5.779848</td>\n",
       "      <td>0.555017</td>\n",
       "      <td>-8.106094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-40.624233</td>\n",
       "      <td>35.157276</td>\n",
       "      <td>-24.591461</td>\n",
       "      <td>-49.362358</td>\n",
       "      <td>8.023435</td>\n",
       "      <td>7.366742</td>\n",
       "      <td>49.222126</td>\n",
       "      <td>-25.602089</td>\n",
       "      <td>34.722832</td>\n",
       "      <td>-19.868265</td>\n",
       "      <td>...</td>\n",
       "      <td>6.108077</td>\n",
       "      <td>28.576767</td>\n",
       "      <td>-23.534607</td>\n",
       "      <td>-16.189983</td>\n",
       "      <td>-31.512745</td>\n",
       "      <td>-27.594158</td>\n",
       "      <td>-26.385250</td>\n",
       "      <td>-12.707217</td>\n",
       "      <td>-22.948502</td>\n",
       "      <td>-21.563004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-22.856718</td>\n",
       "      <td>14.615568</td>\n",
       "      <td>17.575512</td>\n",
       "      <td>6.351932</td>\n",
       "      <td>-3.586997</td>\n",
       "      <td>6.752620</td>\n",
       "      <td>55.943478</td>\n",
       "      <td>-31.262821</td>\n",
       "      <td>4.476537</td>\n",
       "      <td>18.464659</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.847046</td>\n",
       "      <td>31.560469</td>\n",
       "      <td>-41.715862</td>\n",
       "      <td>2.640697</td>\n",
       "      <td>8.316002</td>\n",
       "      <td>-27.561853</td>\n",
       "      <td>-7.074276</td>\n",
       "      <td>15.240105</td>\n",
       "      <td>24.190716</td>\n",
       "      <td>-8.238882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 0  Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  \\\n",
       "0 -25.564545  12.413170   0.916502 -23.523197  23.942314 -19.245277   \n",
       "1 -18.407335  11.812518  -2.667809   6.388415 -18.285372  -4.658720   \n",
       "2  -2.779302   3.396423  -5.502051   8.437631  -8.695304  -5.224719   \n",
       "3 -40.624233  35.157276 -24.591461 -49.362358   8.023435   7.366742   \n",
       "4 -22.856718  14.615568  17.575512   6.351932  -3.586997   6.752620   \n",
       "\n",
       "   Feature 6  Feature 7  Feature 8  Feature 9  ...  Feature 15  Feature 16  \\\n",
       "0  39.597092 -67.066742  27.730808  -9.485415  ...   12.364698   41.769032   \n",
       "1  22.325075 -37.602386  21.859562 -21.498049  ...   -1.258633    8.867926   \n",
       "2  29.446379   2.569920  -7.636728   0.954528  ...    4.210380    5.014014   \n",
       "3  49.222126 -25.602089  34.722832 -19.868265  ...    6.108077   28.576767   \n",
       "4  55.943478 -31.262821   4.476537  18.464659  ...   -7.847046   31.560469   \n",
       "\n",
       "   Feature 17  Feature 18  Feature 19  Feature 20  Feature 21  Feature 22  \\\n",
       "0  -26.896332   23.701748   -1.653188  -19.830172  -27.282768  -11.635708   \n",
       "1   -8.832096   15.507577  -15.484630   -4.390882   11.780195   12.107148   \n",
       "2  -11.005681    5.610911   -7.698394   -2.586307   -0.576156   -5.779848   \n",
       "3  -23.534607  -16.189983  -31.512745  -27.594158  -26.385250  -12.707217   \n",
       "4  -41.715862    2.640697    8.316002  -27.561853   -7.074276   15.240105   \n",
       "\n",
       "   Feature 23  Feature 24  \n",
       "0  -29.070320  -34.822224  \n",
       "1    5.713201  -26.333225  \n",
       "2    0.555017   -8.106094  \n",
       "3  -22.948502  -21.563004  \n",
       "4   24.190716   -8.238882  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.DocVector.apply(pd.Series)\n",
    "X.columns = ['Feature ' + str(i) for i in range(25)]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb07068",
   "metadata": {},
   "source": [
    "Next I need to replace the sentiment labels with numerical dummy values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d1ee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "      <th>DocVector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>0</td>\n",
       "      <td>trend new yorkers encounter empty supermarket ...</td>\n",
       "      <td>[-25.564545, 12.41317, 0.9165025, -23.523197, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>3</td>\n",
       "      <td>could not find hand sanitizer fred meyer turn ...</td>\n",
       "      <td>[-18.407335, 11.812518, -2.667809, 6.388415, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>4</td>\n",
       "      <td>find protect love one</td>\n",
       "      <td>[-2.7793016, 3.3964226, -5.5020514, 8.437631, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>buy hit city anxious shopper stock foodampmedi...</td>\n",
       "      <td>[-40.624233, 35.157276, -24.591461, -49.36236,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>2</td>\n",
       "      <td>everyone buy baby milk powder next everyone bu...</td>\n",
       "      <td>[-22.856718, 14.615568, 17.575512, 6.351932, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...          0   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...          3   \n",
       "2  Find out how you can protect yourself and love...          4   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...          1   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...          2   \n",
       "\n",
       "                                      ProcessedTweet  \\\n",
       "0  trend new yorkers encounter empty supermarket ...   \n",
       "1  could not find hand sanitizer fred meyer turn ...   \n",
       "2                              find protect love one   \n",
       "3  buy hit city anxious shopper stock foodampmedi...   \n",
       "4  everyone buy baby milk powder next everyone bu...   \n",
       "\n",
       "                                           DocVector  \n",
       "0  [-25.564545, 12.41317, 0.9165025, -23.523197, ...  \n",
       "1  [-18.407335, 11.812518, -2.667809, 6.388415, -...  \n",
       "2  [-2.7793016, 3.3964226, -5.5020514, 8.437631, ...  \n",
       "3  [-40.624233, 35.157276, -24.591461, -49.36236,...  \n",
       "4  [-22.856718, 14.615568, 17.575512, 6.351932, -...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements = {'Extremely Negative': 0, 'Negative': 1, 'Neutral' : 2, 'Positive' : 3, 'Extremely Positive' : 4}\n",
    "df['Sentiment'].replace(replacements, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edcd24",
   "metadata": {},
   "source": [
    "The final step in preprocessing is to divide the data into test and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c3e92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6393dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_Corona_NLP.csv')\n",
    "X.to_csv('DocVectors.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
